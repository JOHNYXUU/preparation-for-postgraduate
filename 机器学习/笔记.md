# 第一讲 基本概念 

## 监督学习 supervised learning

数据中已经给出输入对应的答案（标签）

有**回归**（预测连续值）和**分类**（预测离散值）两大问题

## 无监督学习 unsupervised learning

数据中没有给出输入对应的答案（标签）

聚类，自动给数据分类（分簇）

鸡尾酒问题算法，把混合的声源分开

# 第二讲 线性回归模型

## 模型描述

### 训练集 training set

**x** 输入特征

**y** 输出标签

$（x_i,y_i）$表示一个训练样本,第i个

### 学习算法 learning algorithm

### 假设函数h hypothesis 

$h_\theta(x) = \theta_{0} +\theta_{1}x$

## 代价函数

判断假设函数$h_\theta(x) = \theta_{0} +\theta_{1}x$的**优劣**

假设有训练集$(x_i,y_i)$总共m个

我们可以这样定义代价函数$J = \sum\limits_{i=1}^m (h(x_i)-y_i)^2 / 2m$，**se平方误差**

调整参数$\theta$，让代价**J最小化**，是我们的目的

每一个$\theta$对应代价函数的一个值

假如只考虑$\theta_1$

 ![](\图片\QQ截图20210413105804.png)

## 梯度下降 gradient descent

有函数$J(\theta_0,...,\theta_n)$

### 步骤

1 初始化$\theta_i = 0,i=0,1,2,...,n$

2 改变$\theta_i$,得到最小的J 

### 数学原理

$\alpha$是学习率learning rate，控制步子迈的多大

$\alpha$小，更新得慢

$\alpha$过大，可能会不能收敛甚至发散

需要**同时**更新所有的$\theta_i$

![](\图片\QQ截图20210413111706.png)

#### 如何求导

把代价函数求个偏导就好

![](\图片\QQ截图20210413135811.png)

### 最终算法

![](\图片\QQ截图20210413140012.png)

### 问题

会因为初始点的不同，陷入**局部最优**

对于线性回归问题，只有一个最优解，所以不需要关心

# 第三讲 线代回顾

矩阵、向量（$n\times1$矩阵）、矩阵加法（对应项相加，需要维度一致）

矩阵数乘、矩阵向量乘（$m\times n乘n\times 1=m\times1$）、矩阵矩阵乘（$m\times n乘n\times p = m\times p$）

 转置、逆（只有方阵可以有逆矩阵）

# 第四讲 多元梯度下降

 $h_\theta(x)=\theta_0+\theta_1x_1+...+\theta_nx_n$

加一个$x_0 = 1$

 $h_\theta(x)=\theta_0x_0+\theta_1x_1+...+\theta_nx_n=\theta^Tx$

算法如下，就是把n=1的情况推广了一下

 ![](\图片\QQ截图20210413153757.png)

## 实战技巧

### 特征缩放（feature scaling）

假如特征值在**相似**的范围内，梯度下降可以下降得更**快**

#### 原因

特征之间差别过大，会导致梯度下降的时候反复横跳

![](\图片\QQ截图20210413154331.png)

#### 方法

1 除最大值，让特征值缩放到[0,1]之间

![](\图片\QQ截图20210413154627.png)

2 一般让$-1\le x_i\le 1$,比如**均值归一化** mean normalization

![](\图片\QQ截图20210413155038.png)

### 学习率

可以绘制一条代价J和迭代次数n的函数，观察算法是否正常工作

![](\图片\QQ截图20210413160357.png)

假如没有正常下降，就需要调小学习率

![](\图片\QQ截图20210413160600.png)

如果下降过慢，就适当调高学习率

##  特征选择

可以将一些特征组合成新的特征，比如长、宽组成面积

## 多项式回归

将高次的项设成一个一次的项，但需要做数据的缩放（归一化）

![](\图片\QQ截图20210414091347.png)

## 正规方程 （直接解法）

就是线代中的解法

解$X\theta = y$，无解

解$X^TX\theta=X^Ty$

![](\图片\QQ截图20210414092634.png)

### 和梯度下降的比较

梯度下降：

劣势：1 需要设置学习率 2 需要迭代

优势：数据大的时候效果依然很好

正规方程：

优势： 1不需要设置学习率 2 不需要迭代

劣势：数据大的时候很慢$O(n^3)$,上万条数据就可以舍弃了

### 矩阵不可逆的处理方法

#### $X^TX$不可逆的原因

1 特征之间线性相关，比如x1用米来表示长度，x2用英寸表示长度

2 特征比数据多（$m\le n$）

#### 解决方法

1 去掉不需要的特征（线性相关的，过多的）

2 正则化

# 第六讲 分类问题

## 线性回归模型用于分类问题存在问题

1 分的效果不好

2 h（x）可能不在（0,1）之间

![](\图片\QQ截图20210414095706.png)

## 逻辑回归logistic regression解决分类问题

### 假设函数

$0 \le h(x)\le 1$

$h_\theta(x)=g(\theta^Tx)$

$g(z)=\frac{1}{1+e^{-z}}$ **sigmoid function**

![](\图片\QQ截图20210414184036.png)

### 决策边界 decision boundary

设定 y = 1，当 h(x) >= 0.5

​		 y = 0，当h(X) < 0.5

实际上就是判断$\theta^Tx$是否大于0

因为z >0时，g >0.5

![](\图片\QQ截图20210414185130.png)

决策边界指的是把样本分成不同类别的边界，根据$\theta$的取值形成

比如下图的x1+x2=3

![](\图片\QQ截图20210414185647.png)

对于特征中有高维特征的也可以使用

![](\图片\QQ截图20210414190130.png)

### 拟合方法

#### 代价函数

假如使用线性回归的代价函数，得出的函数是非凸函数（有很多的局部最优解）

![](\图片\QQ截图20210414191017.png)

因此要其他的代价函数
$$
cost(h_\theta(x),y)=\left\{
\begin{aligned}
-log(h_\theta(x)),y=1\\
-log(1-h_\theta(x)),y=0
\end{aligned}
\right.
$$
当y=1,h(x)=1，cost = 0，因为预测到了正确的值，h(x)=0,cost=$\infty$，因为预测到了完全错误的值

当y=0，反之

#### 化简代价函数

$cost(h_\theta(x),y)= -ylog(h_\theta(x))-(1-y)log(1-h_\theta(x))$

最终代价函数（借鉴了概率论中的**极大似然估计**）

![](\图片\QQ截图20210414192630.png)

#### 梯度下降

式子是一样的，只是h(x)改了一下

![](\图片\QQ截图20210414193430.png)

### 高级优化

conjugate gradient

bfgs

L-bfgs

不需要设置学习率，比梯度下降更快

但更复杂

### 多类别分类

one vs rest

假设有n个类别，每次都做一次二分类问题，1 为这轮的类别，0为其他

最终的到n个假设函数h(x)

我们取max(h(x))对应的类别

![](\图片\QQ截图20210414195931.png)

# 第七讲 优化方案

## 过拟合overfit问题

欠拟合underfit：得到的h(x)和数据差距（bias）很大

过拟合overfit：过度拟合训练集，高阶特征过多，导致有高方差

### 解决办法

1 减少特征数量

但可能会**丢失**需要的信息，所以需要使用特征选择算法

2 正则化

削减参数的值

## 正则化

### 代价函数

1 给与高阶特征的参数一个惩罚机制，让他们的值很小

比如下图，在代价函数中加了两个很大的值，$1000\theta_3^2,1000\theta_4^2$

假如想让代价函数很小的话，$\theta_3,\theta_4$必然会很小

也就会得到一个相对简单的假设函数h，让函数更加平滑

![](\图片\QQ截图20210414202649.png)

2 对于不清楚那些特征是高阶的，或者只是特征比较多

在代价函数后面加一个所有参数的平方和

![](\图片\QQ截图20210414203329.png)

### $\lambda$ 的意义

$\lambda$ 是惩罚程度

过大的话，参数会都趋向0，相当于把特征全部删除了，会欠拟合

### 线性回归中的正则化

#### 梯度下降

把新的代价函数求个导

![](\图片\QQ截图20210414204645.png)

#### 正规方程

在$X^TX$右侧加一个对角矩阵，可以保证得到的矩阵可逆（不会存在线性相关的列）

![](\图片\QQ截图20210414205655.png)

### 逻辑回归的正则化

同样是在代价函数上加上参数的平方和

![](\图片\QQ截图20210414211145.png)

得到的结果依然和线性回归很像，只是h(x)不一样

![](\图片\QQ截图20210414211202.png)

# 第八讲 神经网络

## 使用原因

非线性假设（特征多且阶数高）问题中，线性回归和逻辑回归算起来很慢。

## 模型表示

**单个神经元**

h(x) = g(z)

g 为激活函数activation function

![](\图片\QQ截图20210416091838.png)

**多个神经元**

输入层、隐藏层、输出层

![](\图片\QQ截图20210416092204.png)

每一个神经元中都是一个逻辑回归

![](\图片\QQ截图20210416092419.png)

## 前向传播

使用矩阵乘的方式计算神经元的值，前一层的值$a^{(j-1)}$作为$x$,这一层的参数写成矩阵$\theta^{(j)}$

相乘$\theta^{(j)}x=a^{(j)}$

![](\图片\QQ截图20210416094636.png)

## 多分类

输出四个神经元的值的向量

![](\图片\QQ截图20210416104255.png)

## 代价函数

假设分成K类

还要加上正则项

$s_l$指的是第$l$层神经元的个数

![](\图片\QQ截图20210416105637.png)

## 反向传播

为了方便计算偏导数，**从后往前**，计算每一层的$\delta$

![](\图片\QQ截图20210416112458.png)

使用每一层的$\delta$

计算偏导数，在使用偏导数经行梯度下降，或者其他算法

![](\图片\QQ截图20210416112754.png)

##  梯度检测

 计算该处梯度的近似值，和计算出的比较

![](\图片\QQ截图20210416143353.png)

![](\图片\QQ截图20210416143727.png)

## 随机初始化

$\theta$的初始值使用随机数来初始化

假如都用0来初始化，反向传播的结果都是一样的

## 整体实现

1 输入层就是$x_i$,输出层就是y，有几类，输出层就有几个神经元，中间层的神经元个数一般**个数一致**（通常越多越好）

2 随机设置权重$\theta$

3 前向传播

4 计算代价函数

5 后向传播计算偏导

6 梯度检查

7 梯度下降（或其他优化算法optimization）

# 第九讲 提升性能

## 评估假设函数

将一部分数据作为**训练集**，一部分用作**测试集**

比例7:3之类的

需要事先打乱顺序（shuffle）

对于**回归问题**，我们可以计算测试集的均方误差，来衡量模型的优劣。

对于**分类问题**，我们可以计算测试集的错误率

## 验证集

把数据分成三个部分，训练集、验证集、测试集（6：2：2）

用训练集训练权重

用验证集判断模型误差的大小

用测试集判断模型的泛化能力

## 偏差、方差、正则化

偏差大：欠拟合

方差大：过拟合

验证误差大，训练误差也大，则欠拟合

验证误差大，训练误差小，则过拟合

![](\图片\QQ截图20210416163912.png)

正则参数选择，$\lambda$**过小会过拟合，过大会欠拟合**

![](\图片\QQ截图20210416165630.png)

## 学习曲线

正常来说，训练误差（蓝色）验证误差（玫红色）曲线如下图

![](\图片\QQ截图20210416191500.png)

高偏差（欠拟合）时，两个误差都很大，并且哪怕训练集增加，也几乎不会让验证误差下降

![](\图片\QQ截图20210416191924.png)

高方差（过拟合），差别会很大，增加数据在过拟合问题中是有效的

![](\图片\QQ截图20210416192245.png)

## 总结

过拟合：1 获得更多数据 2 减少特征3 增加$\lambda$

欠拟合：1 加额外的特征 2 加高阶的特征 3 减小$\lambda$

# 第十讲 常见问题和解决

## 误差分析

在验证集手动分析判断错误的数据，增加一些可能的数据（特征）

## 不对称性分类的误差评估

不对称指的是数据中y=0，和y=1的个数差别很大，比如99.5%的数据都是y=0

这时候只用准确率来衡量模型就不合适了

### 召回率recall/查准率precision

将真实的输出和预测的输出比较，分为四类，真阳性、真阴性、假阳性、假阴性，计算他们的个数

![](\图片\QQ截图20210416204948.png)

#### **查准率**P

$p=\frac{true\ pos}{predict\ pos}$，p越大越好，意思是判定为真的数据中有多少真的为真

 ![](\图片\QQ截图20210416205211.png)

#### **召回率**R

也是越高越好

意思是输出为真的数据中有多少被判断为真了

![](\图片\QQ截图20210416205846.png)

#### **提高查准率**

把阈值提高，比如设h(x)>0.9,y=1，但召回率会降低

#### **提高召回率**

把阈值降低，比如设h(x)>0.3,y=1，但查准率会降低

### F1-score

结合了P和R，给出一个单一的指标

$F_1 = 2\frac{PR}{P+R}$

# 第十一讲 支持向量机SVM

## 代价函数

cost1

x>=1的时候为0，前面的和逻辑回归的代价函数相似，但是是一条直线

![](\图片\QQ截图20210417093051.png)

cost2

x<=-1的时候为0，后面的和逻辑回归的代价函数相似，但是是一条直线

![](\图片\QQ截图20210417093147.png)

最终结果，乘c当做权重，可以理解为正则化常数

![](\图片\QQ截图20210417092959.png)

## 假设函数

![](\图片\QQ截图20210417093428.png)

## 大间距分类器

svm的决策边界和数据有较大的边界，以最大的边界把正负样本分开

如图中黑线

![](\图片\QQ截图20210417094409.png)

假如C设的太大，会导致对于奇异点过于敏感，如图玫红色

![](\图片\QQ截图20210417094816.png)

## 核函数（高斯kernel）

相似度：两个点a,b的相似度用如下式子表示 

$exp(-\frac{||a-b||^2}{2\sigma^2})$

### 步骤

1 选择一些点l(1)、l(2)...作为标记点

2 $f_i = similarity(x,l^{(i)}) = exp(-\frac{||x-l^{(i)}||^2}{2\sigma^2})$

3 核函数 =$\theta_0+\theta_1f_1+\theta_2f_2+\theta_3f_3\ge0$,则预测结果为 1

###  如何选择标记点

可以使用训练集中的点作为标记点，假设有m个测试样例

对于给定的测试样例$x$,我们可以计算出它m个标记点的相似度$f_0=1,f_1,f_2,...,f_m$，记为向量$f$

假设此时$\theta$已经计算出来，就可以直接预测结果了

$\theta^Tf\ge0,y=1$

### 使用核函数后的代价函数

将原来的$x_i$,换成$f_i$，n=m

![](\图片\QQ截图20210417110543.png)

## 参数设定

$C = \frac{1}{\lambda}$

C太大，过拟合，方差大、偏差小

C太小，欠拟合，方差小、偏差大

$\sigma^2$

$\sigma^2$大，$f_i$变化得更平滑，偏差大、方差小，欠拟合

$\sigma^2$小，$f_i$变化得更剧烈，偏差小、方差大，过拟合

![](\图片\QQ截图20210417111818.png)

## 多分类问题

同样是one vs others方法

## 模型选择

n = 特征数，m = 数据数

1    n>=m,使用逻辑回归或者无核的svm（linear kernel就用$x_i$当做$f_i$）

2   n小，m不是特别大，可以用高斯kernel的svm（因为计算量大）

3   n小，m特别大，增加特征，使用逻辑回归或者无核的svm（linear kernel就用$x_i$当做$f_i$）

# 第十二讲 无监督学习

## 聚类算法 clustering algorithm--K-means

### 步骤

1 随机取两个（假如分成两类的话）聚类点

2 对于每一个点，计算到两个聚类点的距离，距离第一个点近的标成一类，距离第二个点近的标成一类

3 将聚类点更新成它那一类的点的均值

4 重复2-3

### 问题

1  假如有一个点最终周围没有点，则一般把**这个点删除**  或者  **重新随机点再做一次k-means**

2 簇不是特别明显,就正常使用算法，再根据分出的簇手动打上标签

![](\图片\QQ截图20210417152721.png)

### 代价函数

每个点到该点聚类中心的距离平方和

$x^{(i)}$表示训练数据 i 

$c^{(i)}$表示第 i 个训练数据的聚类中心的下标

$\mu_k$表示第k个聚类中心

![](\图片\QQ截图20210417153306.png)

### 随机初始化

随机选择k个样本点作为初始化聚类中心

多次随机初始化，避免局部最优

对于每一次，都计算其代价函数，选择代价最低的那个

### 确定k的数值

肘部算法

取拐点处的k的值

![](\图片\QQ截图20210417155801.png)

假如下降地比较平稳，找不到拐点，选起来就比较困难，所以这个算法并不可靠

所以一般按照需要解决的问题来找到合适的k

## 降维

### 用处

#### 数据压缩

把点降维到低维的平面上

之前描述一个点需要$(x_1,x_2,x_3)$现在只需要$(z_1,z_2)$

可以减少内存和硬盘的开销，加速算法

 ![](\图片\QQ截图20210417161651.png)

#### 数据可视化

对于多维的数据，想要可视化在图上，就需要降维到低维

### 主成分分析算法PCA

PCA = princial component analysis

一种降维的算法，通过投影降维，使投影后的误差最小（并不是线性回归！！！）

PCA计算的距离是x点到直线的距离，并且所有的特征是被同等对待的，没有一个预测值

而线性回归计算的是垂直距离

![](\图片\QQ截图20210417165503.png)

#### 步骤

1 每个特征减去该特征的平均值,再缩放

$s_j$是标准差

$\mu_j=\frac{1}{m}\sum\limits_{i=1}^{m}x_j^{(i)}$

$x_j^{(i)} = (x_j^{(i)} - \mu_j)/s_j$

2 将n维降到k维

（1）首先计算协方差矩阵$\Sigma = \frac{1}{m}\sum\limits_{i=1}^m(x^{(i)})(x^{(i)})^T$

（2）计算协方差矩阵的特征向量，进行奇异值分解SVD $[U,S,V]=svd(\Sigma)$

（3）得到的U是一个$n\times n$的矩阵，取前k个列向量，就是我们投影的方向

#### 如何确定k

也就是投影后对所有数据的影响很小

这个例子中也就是说99%的方差被保留下来了

可以使用该方法，遍历k的值，看k到几可以满足需求

![](\图片\QQ截图20210417173209.png)

 但这样的计算太大了，可以利用SVD中的对角矩阵S来计算这个值

$1 - \frac{\sum\limits_{i=1}^{k}S_{ii}}{\sum\limits_{i=1}^{m}S_{ii}}$（奇异值求和）和上面的值相等

![](\图片\QQ截图20210417173930.png)

#### 压缩重现

如何把压缩后的数据还原

乘逆矩阵就好

#### 应用建议

##### 加速监督学习

对于高维的输入数据，先进行pca，在进行训练

**记住，只对训练集进行pca，再把计算出的映射规则运用于验证集和测试集！！！**

##### 错误使用

1 不能用来解决过拟合

2 不要一上来就用pca，先用原始数据

# 第十三讲 异常检测

 检测数据中的异常值

## 高斯（正态）分布

$x\sim N(\mu,\sigma^2)$

 假设有n个数据$x_1,...,x_n$

极大似然估计$\mu = \frac{1}{m}\sum\limits_{i=1}^{m}x_i$，$\sigma^2 = \frac{1}{m}\sum\limits_{i=1}^{m}(x_i-\mu)^2$

## 算法

1 假设有m条数据，每条数据有n个特征，我们认为每个特征的值都符合高斯分布$x_i\sim N(\mu_i,\sigma_i^2)$

2 通过极大似然估计算出$\mu_i,\sigma_i^2$

3  $p(x)=p(x_1;\mu_1,\sigma_1^2)P(x_2;\mu_2,\sigma_2^2)...p(x_n;\mu_n,\sigma_n^2)=\prod\limits_{j=1}^n p(x_j;\mu_j,\sigma_j^2)$

 当$p(x)<\epsilon$,认为异常

## 实际操作

1 假设有 m 条数据，其中标记了是否异常，往往异常的非常少

2 按比例，从正常的数据中划分出训练集、验证集、测试集，把异常的按比例放入验证集和测试集

3 使用训练集计算p(x),计算f1-score，使用验证集确定$\epsilon$

## 和监督学习比较

异常样本太少，监督学习算法很难学习到什么是异常，这时候异常检测算法就比较合适

## 特征处理

对于一些不是高斯分布的特征，我们想办法把他们转换成高斯分布，比如套个log

![](\图片\QQ截图20210417203114.png)

使用可以区分的特征方便捕捉异常

## 多元高斯分布

### 遇到的问题

一元高斯分布对于异常的检测往往是轴对齐的，比如图中的绿叉其实是异常的 ，但是由于检测效果类似一个圆，所以没有被认为特别差

![](\图片\QQ截图20210417204857.png)

 使用协方差矩阵表示多元高斯分布p(x)，原来的方法也可用这个式子表示，只是除了对角线上都是0，所以表现出轴对齐的现象

![](\图片\QQ截图20210417210349.png)

![](\图片\QQ截图20210417210138.png)

非对角线上值不为0的时候，可以表现出非轴对齐的效果

 ![](\图片\QQ截图20210417205629.png)

### 与原模型比较

1 原模型计算量小，多元高斯模型计算量大

2 m>n的时候才能用多元高斯模型，否则协方差函数不可逆

# 第十四讲 推荐系统

##  基于内容的推荐系统

假设有电影的一些特征

设$y^{(i,j)}$ 为用户 j 对电影 i 的评价

$\theta^{(j)}$ 为用户 j 的参数向量

$x^{(i)}$ 是电影 i 的特征向量

类似于线性回归，我们预测用户 j 对电影 i 的评价为$(\theta^{(j)})^T(x^{(i)})$

代价函数为

![](\图片\QQ截图20210418150017.png)

对所有用户

![](\图片\QQ截图20210418150142.png)

梯度下降

![](\图片\QQ截图20210418150326.png)

## 协同过滤

假设知道用户的喜好，去预测电影的特征

r(i,j)代表用户 j 是否对电影 i 打过分

![](\图片\QQ截图20210418151852.png)

将两种方法结合

![](\图片\QQ截图20210418152041.png)

### 步骤

1 随机初始化x(i),$\theta^{(i)}$

2 梯度下降

3 预测用户的评分$(\theta^{(j)})^T(x^{(i)})$

###  推荐方法

假设已经计算出两部电影的特征值x(i),x(j)

假如$||x(i) - x(j)||$足够小，则推荐

### 均值归一化

假如有一个用户没有对任何一部电影评分，依然使用上述方法，会认为他会给所有电影打0分（因为训练出的参数都是0），显然不合适

对于所有已经打过的分，我们减去这部电影的均值

![](\图片\QQ截图20210418154745.png)

再进行上面的方法，但预测的时候加上均值

![](\图片\QQ截图20210418154931.png)

这样对于没有打过分的人对某部电影的评分，就会预测为这部电影的均值了

# 第十五讲 大规模数据处理

##  随机梯度下降SGD

数据很大的话，普通的梯度下降就算起来很慢了

### 步骤

1 打乱数据集

2 对每一个训练集的数据，只根据这一个数据做梯度下降

m 为数据量，n为特征数，重复1~10次即可（一般一次就好）

![](\图片\QQ截图20210418161635.png)

收敛的过程比较随机，但总体是朝向最优解的

![](\图片\QQ截图20210418162129.png)

### 检查梯度

每隔1000次迭代，在更新参数前，计算一下对于这个样本的代价

### 技巧

逐步减小学习率

![](\图片\QQ截图20210418164438.png)

### 运用：在线学习

对于每一个用户的特征x，和他的决策y，更新模型中的参数$\theta$，仅用一次

可以捕捉到用户偏好的改变

## mini-batch 梯度下降

batch 梯度下降（最先接触的）：每次迭代使用m个样本

SGD随机梯度下降：每次迭代使用1个样本

mini-batch 梯度下降：每次迭代使用1<b<m个样本

![](\图片\QQ截图20210418162656.png)

## map reduce 和 数据并行

### 原因

数据太大，需要在几台电脑上运行模型

### 步骤

1 把数据分成几份

每一台电脑对其中一份进行计算梯度

![](\图片\QQ截图20210418170941.png)

最后交给一台总的电脑进行这次对参数$\theta$的调整

![](C:\Users\11634\Desktop\保研准备\课程学习\机器学习\图片\QQ截图20210418171220.png)

# 第十六讲 OCR

## 工作流程 pipeline

1 获得图片

2 检测文字的区域

3 字母分隔

4 字母分类（classification）

## 滑动窗口

1 首先找一些行人和非行人的照片（长宽一致），让一个模型去学习分类行人和非行人

2 在需要检测的照片上滑动一个窗口，检测是否有行人的，可以使用更大的框

![](\图片\QQ截图20210418173456.png)

文字则需要对检测到的部分放大，就是白色的部分

![](\图片\QQ截图20210418174026.png)



3 训练一个分类器，分辨是否可以作为分隔

![](\图片\QQ截图20210418174438.png)

在图片上滑动窗口，绘制分隔

![](\图片\QQ截图20210418174613.png)

## 人工合成数据

比如字体识别：可以把字体和一些随机的背景结合，当做训练数据

把原有数据做一些手动的调整、噪声

![](\图片\QQ截图20210418175230.png)

## 上限分析

### 目的

决定pipeline中提高哪一个效果最好

### 步骤

给与每一步正确的结果，看最后结果提高了多少

比如不让机器来检测文本，而是直接给出检测的图片到下一步，可以看到准确率提高到89%

但给与正确的分隔，只提高了1%

所以可以多花点精力在文本检测上

![](\图片\QQ截图20210418180246.png)

